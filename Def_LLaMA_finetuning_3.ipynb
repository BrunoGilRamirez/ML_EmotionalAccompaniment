{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BrunoGilRamirez/ML_EmotionalAccompaniment/blob/main/Def_LLaMA_finetuning_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WuAlTCs2b-Y"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-08-09T19:41:24.782455Z",
          "iopub.status.busy": "2023-08-09T19:41:24.782038Z",
          "iopub.status.idle": "2023-08-09T19:42:16.200880Z",
          "shell.execute_reply": "2023-08-09T19:42:16.200118Z",
          "shell.execute_reply.started": "2023-08-09T19:41:24.782424Z"
        },
        "id": "pGB11PvSZkZp",
        "outputId": "d415e4ff-780e-48d7-9594-8c0e5e37c09d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/lvwerra/trl.git\n",
            "  Cloning https://github.com/lvwerra/trl.git to /tmp/pip-req-build-6skznwda\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/lvwerra/trl.git /tmp/pip-req-build-6skznwda\n",
            "  Resolved https://github.com/lvwerra/trl.git to commit 98120d6aeb104e2b9d4e998774dfa0518a9bd0fa\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from trl==0.5.1.dev0) (2.0.1+cu118)\n",
            "Collecting transformers>=4.18.0 (from trl==0.5.1.dev0)\n",
            "  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m53.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.18.2 in /usr/local/lib/python3.10/dist-packages (from trl==0.5.1.dev0) (1.23.5)\n",
            "Collecting accelerate (from trl==0.5.1.dev0)\n",
            "  Downloading accelerate-0.21.0-py3-none-any.whl (244 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets (from trl==0.5.1.dev0)\n",
            "  Downloading datasets-2.14.4-py3-none-any.whl (519 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.3/519.3 kB\u001b[0m \u001b[31m50.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl==0.5.1.dev0) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl==0.5.1.dev0) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl==0.5.1.dev0) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl==0.5.1.dev0) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl==0.5.1.dev0) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl==0.5.1.dev0) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.4.0->trl==0.5.1.dev0) (3.27.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.4.0->trl==0.5.1.dev0) (16.0.6)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers>=4.18.0->trl==0.5.1.dev0)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.18.0->trl==0.5.1.dev0) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.18.0->trl==0.5.1.dev0) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.18.0->trl==0.5.1.dev0) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.18.0->trl==0.5.1.dev0) (2.31.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers>=4.18.0->trl==0.5.1.dev0)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m75.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers>=4.18.0->trl==0.5.1.dev0)\n",
            "  Downloading safetensors-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m84.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.18.0->trl==0.5.1.dev0) (4.66.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate->trl==0.5.1.dev0) (5.9.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->trl==0.5.1.dev0) (9.0.0)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets->trl==0.5.1.dev0)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->trl==0.5.1.dev0) (1.5.3)\n",
            "Collecting xxhash (from datasets->trl==0.5.1.dev0)\n",
            "  Downloading xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets->trl==0.5.1.dev0)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets->trl==0.5.1.dev0) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->trl==0.5.1.dev0) (3.8.5)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl==0.5.1.dev0) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl==0.5.1.dev0) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl==0.5.1.dev0) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl==0.5.1.dev0) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl==0.5.1.dev0) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl==0.5.1.dev0) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl==0.5.1.dev0) (1.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.18.0->trl==0.5.1.dev0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.18.0->trl==0.5.1.dev0) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.18.0->trl==0.5.1.dev0) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.4.0->trl==0.5.1.dev0) (2.1.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->trl==0.5.1.dev0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->trl==0.5.1.dev0) (2023.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.4.0->trl==0.5.1.dev0) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets->trl==0.5.1.dev0) (1.16.0)\n",
            "Building wheels for collected packages: trl\n",
            "  Building wheel for trl (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for trl: filename=trl-0.5.1.dev0-py3-none-any.whl size=90464 sha256=3f7da8c19eb0efa11d506074fb4f584b6547ad3e0926383979331c4739228c08\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-dpb2p7im/wheels/29/2a/0c/4cad07713d956ee7cbee0654570e3f3ccc30da3da04742db5d\n",
            "Successfully built trl\n",
            "Installing collected packages: tokenizers, safetensors, xxhash, dill, multiprocess, huggingface-hub, transformers, datasets, accelerate, trl\n",
            "Successfully installed accelerate-0.21.0 datasets-2.14.4 dill-0.3.7 huggingface-hub-0.16.4 multiprocess-0.70.15 safetensors-0.3.2 tokenizers-0.13.3 transformers-4.31.0 trl-0.5.1.dev0 xxhash-3.3.0\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m90.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.5/188.5 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.6/215.6 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -U git+https://github.com/lvwerra/trl.git\n",
        "!pip install -q -U datasets bitsandbytes einops wandb torch\n",
        "# peft es una libreria para calcular la eficiencia de la aceleración de un modelo\n",
        "!pip install -U git+https://github.com/huggingface/peft.git\n",
        "# transformers es una libreria para entrenar y usar modelos de NLP\n",
        "!pip install -U transformers\n",
        "!pip install -U tokenizers\n",
        "# sentencepiece es una libreria para tokenizar texto en subpalabras\n",
        "!pip install -U seaborn\n",
        "# accelerate es una libreria de huggingface para acelerar el entrenamiento de modelos de NLP en GPU y TPU\n",
        "!pip install -U accelerate\n",
        "!pip install -U evaluate\n",
        "!pip install -U bitsandbytes\n",
        "!pip install -U git+https://github.com/huggingface/huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-09T19:45:14.849423Z",
          "iopub.status.busy": "2023-08-09T19:45:14.848747Z",
          "iopub.status.idle": "2023-08-09T19:45:14.855924Z",
          "shell.execute_reply": "2023-08-09T19:45:14.854996Z",
          "shell.execute_reply.started": "2023-08-09T19:45:14.849396Z"
        },
        "id": "3xG6yyBJbW4N"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")\n",
        "import transformers # transformers es de hugingface\n",
        "from transformers import LlamaTokenizer, LlamaForCausalLM, AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig # LlamaTokenizer y LlamaForCausalLM son clases de transformers\n",
        "import os # os es una libreria para interactuar con el sistema operativo\n",
        "import sys # sys es una libreria para interactuar con el sistema operativo\n",
        "import wandb\n",
        "from peft import ( # peft es de hugingface\n",
        "    LoraConfig, # LoraConfig es una clase de peft que contiene la configuración de Lora, Lora es un modelo de NLP que usa transformers y llama como tokenizer\n",
        "    get_peft_model, # get_peft_model es una función de peft que obtiene el modelo de Lora\n",
        "    get_peft_model_state_dict, # get_peft_model_state_dict es una función de peft que obtiene el estado del modelo de Lora\n",
        "    prepare_model_for_kbit_training, # prepare_model_for_int8_training es una función de peft que prepara el modelo de Lora para el entrenamiento de int8\n",
        "    PeftModel\n",
        ")\n",
        "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n",
        "import torch # torch es una libreria para entrenar y usar modelos de NLP\n",
        "import datasets  # datasets es una libreria para cargar y procesar conjuntos de datos de NLP\n",
        "import pandas as pd # pandas es una libreria para análisis de datos\n",
        "from huggingface_hub import login # huggingface_hub es de hugingface\n",
        "import matplotlib.pyplot as plt # matplotlib es una libreria para visualización de datos\n",
        "import pandas as pd # pandas es una libreria para análisis de datos"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "OUTPUT_DIR = \"/content/gdrive/MyDrive/Tesis/Experimentacion/puntos-de-control\" # directorio de salida de los experimentos\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\" #   se muestra si hay una GPU disponible, si no, se muestra la CPU\n",
        "DEVICE"
      ],
      "metadata": {
        "id": "s2RB5ElwoaG-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-09T19:20:46.789795Z",
          "iopub.status.busy": "2023-08-09T19:20:46.788435Z",
          "iopub.status.idle": "2023-08-09T19:20:46.840717Z",
          "shell.execute_reply": "2023-08-09T19:20:46.840073Z",
          "shell.execute_reply.started": "2023-08-09T19:20:46.789759Z"
        },
        "id": "eaIf1UP519PM"
      },
      "outputs": [],
      "source": [
        "access_token = \"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\"\n",
        "login(token=access_token)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AWsbZI0ZL4N"
      },
      "source": [
        "## Cargar el modelo LLaMA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-09T19:20:49.334998Z",
          "iopub.status.busy": "2023-08-09T19:20:49.334711Z",
          "iopub.status.idle": "2023-08-09T19:20:49.340993Z",
          "shell.execute_reply": "2023-08-09T19:20:49.340348Z",
          "shell.execute_reply.started": "2023-08-09T19:20:49.334975Z"
        },
        "id": "JhZ2c9di19PN"
      },
      "outputs": [],
      "source": [
        "parameters=\"7b-chat\"\n",
        "BASE_MODEL = f\"meta-llama/Llama-2-{parameters}-hf\" # modelo base de llama de 7B de parámetros\n",
        "# if there is a pretrained model, load it the model is Models_of_Llama/Llama_base\n",
        "pathBase = f\"Models_of_Llama/Llama_base_{parameters}\"#base path\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-09T19:28:58.430293Z",
          "iopub.status.busy": "2023-08-09T19:28:58.430004Z",
          "iopub.status.idle": "2023-08-09T19:29:02.431056Z",
          "shell.execute_reply": "2023-08-09T19:29:02.430394Z",
          "shell.execute_reply.started": "2023-08-09T19:28:58.430272Z"
        },
        "id": "SvgJvcO9d12M"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL,\n",
        "    #torch_dtype=torch.float16,\n",
        "    quantization_config=bnb_config,\n",
        "    #load_in_8bit=True,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "model.tie_weights()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-09T19:46:25.682490Z",
          "iopub.status.busy": "2023-08-09T19:46:25.681761Z",
          "iopub.status.idle": "2023-08-09T19:46:25.808861Z",
          "shell.execute_reply": "2023-08-09T19:46:25.807934Z",
          "shell.execute_reply.started": "2023-08-09T19:46:25.682462Z"
        },
        "id": "S0FfSdLN19PP"
      },
      "outputs": [],
      "source": [
        "mymodel='BrunoGR/LLaMA-2-7bChat-modified'\n",
        "tokenizer =   AutoTokenizer.from_pretrained(mymodel)\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.resize_token_embeddings(len(tokenizer.get_vocab()))"
      ],
      "metadata": {
        "id": "W5u2iLr2NZge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-09T19:26:40.673040Z",
          "iopub.status.busy": "2023-08-09T19:26:40.672755Z",
          "iopub.status.idle": "2023-08-09T19:26:40.677010Z",
          "shell.execute_reply": "2023-08-09T19:26:40.676322Z",
          "shell.execute_reply.started": "2023-08-09T19:26:40.673017Z"
        },
        "id": "0l21m2DU19PQ"
      },
      "outputs": [],
      "source": [
        "def response (query:str,maxtoken:int):\n",
        "    input_ids = tokenizer(query, return_tensors=\"pt\").input_ids.to('cuda')\n",
        "\n",
        "    generation_output = model.generate(\n",
        "        input_ids=input_ids, max_new_tokens=maxtoken\n",
        "    )\n",
        "    out = tokenizer.decode(generation_output[0], skip_special_tokens=True)\n",
        "    print(out)\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-09T19:26:43.353682Z",
          "iopub.status.busy": "2023-08-09T19:26:43.353402Z",
          "iopub.status.idle": "2023-08-09T19:26:46.834011Z",
          "shell.execute_reply": "2023-08-09T19:26:46.833396Z",
          "shell.execute_reply.started": "2023-08-09T19:26:43.353660Z"
        },
        "id": "rI3hLym719PQ"
      },
      "outputs": [],
      "source": [
        "prompt = \"Eres un asistente que habla español:\\n Consulta:hola!como estás?\\nRespuesta:\"\n",
        "a=response(prompt,22)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer(\"Eres un asistente que habla español:\\n Consulta:hola!como estás?\\nA:\")"
      ],
      "metadata": {
        "id": "ea8r48IBFGsV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HzSlXm1_T3V5"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data= datasets.load_dataset(\"BrunoGR/Emo_support\")"
      ],
      "metadata": {
        "id": "iVC72e5WM8Mb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-06T09:02:50.233483Z",
          "iopub.status.busy": "2023-08-06T09:02:50.233325Z",
          "iopub.status.idle": "2023-08-06T09:02:50.238924Z",
          "shell.execute_reply": "2023-08-06T09:02:50.238217Z",
          "shell.execute_reply.started": "2023-08-06T09:02:50.233461Z"
        },
        "id": "D7kaRDNqg3aB"
      },
      "outputs": [],
      "source": [
        "def generate_prompt(data_point):\n",
        "#limita hasta 1024 tokens el texto\n",
        "    text = data_point[\"texto\"][:2460]\n",
        "\n",
        "    return {'Prompt':f\"Intruccion: Analiza la Consulta, y determina la Emocion.\\n {text} \\n### {data_point['etiqueta']}\"}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-06T09:02:50.242140Z",
          "iopub.status.busy": "2023-08-06T09:02:50.241933Z",
          "iopub.status.idle": "2023-08-06T09:03:29.863082Z",
          "shell.execute_reply": "2023-08-06T09:03:29.862498Z",
          "shell.execute_reply.started": "2023-08-06T09:02:50.242120Z"
        },
        "id": "OPmfOpQQtDMN"
      },
      "outputs": [],
      "source": [
        "train_data = (data[\"train\"].map(generate_prompt)) # se obtienen los datos de entrenamiento y se tokenizan con el tokenizer de llama"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data['Prompt'][1]"
      ],
      "metadata": {
        "id": "wetmFBZK55ty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Original:\\n{train_data['Prompt'][1]}\\ntokenizado{tokenizer.tokenize(train_data['Prompt'][1])}\\n codificado:{tokenizer.encode(train_data['Prompt'][1])}\")"
      ],
      "metadata": {
        "id": "msv9GcYA5vbC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-06T09:03:29.865517Z",
          "iopub.status.busy": "2023-08-06T09:03:29.865335Z",
          "iopub.status.idle": "2023-08-06T09:03:39.525984Z",
          "shell.execute_reply": "2023-08-06T09:03:39.525265Z",
          "shell.execute_reply.started": "2023-08-06T09:03:29.865464Z"
        },
        "id": "tFUuNWdk19PS"
      },
      "outputs": [],
      "source": [
        "test_data = (data[\"test\"].map(generate_prompt))  # lo mismo para los datos de prueba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-06T09:03:39.528954Z",
          "iopub.status.busy": "2023-08-06T09:03:39.528742Z",
          "iopub.status.idle": "2023-08-06T09:03:39.554122Z",
          "shell.execute_reply": "2023-08-06T09:03:39.553557Z",
          "shell.execute_reply.started": "2023-08-06T09:03:39.528932Z"
        },
        "id": "Vxe14F_319PT"
      },
      "outputs": [],
      "source": [
        "val_data = (data[\"validation\"].map(generate_prompt))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-06T09:03:39.594644Z",
          "iopub.status.busy": "2023-08-06T09:03:39.594401Z",
          "iopub.status.idle": "2023-08-06T09:03:46.453576Z",
          "shell.execute_reply": "2023-08-06T09:03:46.452817Z",
          "shell.execute_reply.started": "2023-08-06T09:03:39.594620Z"
        },
        "id": "6-cTLirahjfg"
      },
      "outputs": [],
      "source": [
        "model.config.pretraining_tp = 1\n",
        "LoRA_TARGET_MODULES = [ # Esta lista especifica los módulos del modelo de lenguaje original que se adaptarán mediante la técnica LoRA\n",
        "    \"q_proj\", # q_proj es la proyección de consulta\n",
        "    \"v_proj\", # v_proj es la proyección de valor\n",
        "]\n",
        "\n",
        "LoRA_DROPOUT= 0.05\n",
        "config = LoraConfig( # se configura el modelo de llama\n",
        "    r=16, # indica el número de factores o dimensiones principales utilizados en la descomposición de las matrices de peso del modelo de lenguaje original.\n",
        "    lora_alpha=32,\n",
        "    target_modules=LoRA_TARGET_MODULES,\n",
        "    lora_dropout=LoRA_DROPOUT,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "#model_train = get_peft_model(model_train, config) # se obtiene el modelo de llama\n",
        "#model_train.print_trainable_parameters() # se muestran los parámetros entrenables del modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBmdF1XJ7ZOu"
      },
      "source": [
        "# Fine Tune\n",
        "En esta seccion se hara un ajuste al modelo con los datos de Emo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmHP1xr719PU"
      },
      "source": [
        "## Argumentos del entrenamiento\n",
        "En esta sección se configura el entrenamiento con la función de entrenamiento de transformers y se configuran los argumentos de entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-06T09:03:46.469026Z",
          "iopub.status.busy": "2023-08-06T09:03:46.468882Z",
          "iopub.status.idle": "2023-08-06T09:03:46.484369Z",
          "shell.execute_reply": "2023-08-06T09:03:46.483784Z",
          "shell.execute_reply.started": "2023-08-06T09:03:46.469009Z"
        },
        "id": "-urSep2JDVH0"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 256 # tamaño del batch, es decir, cuantos textos se procesan a la vez\n",
        "MICRO_BATCH_SIZE = 16# tamaño del micro batch, es decir, cuantos textos se procesan a la vez en la GPU\n",
        "GRADIENT_ACCUMULATION_STEPS = BATCH_SIZE // MICRO_BATCH_SIZE # pasos de acumulación de gradientes\n",
        "training_arguments = transformers.TrainingArguments( # se configuran los argumentos de entrenamiento\n",
        "    per_device_train_batch_size=MICRO_BATCH_SIZE, # tamaño del micro batch\n",
        "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS, # pasos de acumulación de gradientes\n",
        "    warmup_steps=200, # pasos de calentamiento del entrenamiento\n",
        "    num_train_epochs = 6, # epocas de entrenamiento que son 300\n",
        "    learning_rate=5e-5, # tasa de aprendizaje\n",
        "    adam_beta1=0.9, # betas de adam, se usa el mismo del paper de llama\n",
        "    adam_beta2=0.95, # se usa el mismo del paper de llama\n",
        "    adam_epsilon=1e-8, # se usa el mismo del paper de llama\n",
        "    weight_decay=0.1,\n",
        "    fp16=True, # se usa la precisión de 16 bits\n",
        "    logging_steps=10, # pasos de logging\n",
        "    optim=\"adamw_torch\", # optimizador adamw, se usa el de torch\n",
        "    evaluation_strategy=\"steps\", # estrategia de evaluación\n",
        "    save_strategy=\"steps\", # estrategia de guardado\n",
        "    eval_steps=100, # cada 50 pasos se evalúa el modelo\n",
        "    save_steps=100, # cada 50 pasos se guarda el modelo\n",
        "    output_dir=OUTPUT_DIR, # directorio de salida\n",
        "    save_total_limit=6, # límite de guardado3\n",
        "    load_best_model_at_end=True, #se guarda el mejor modelo al final\n",
        "    report_to=\"wandb\", # se reporta a tensorboard\n",
        "    seed=1,\n",
        "    lr_scheduler_type = \"cosine\",# tal y como dice en el paper de llama\n",
        "    max_grad_norm = 1.0, # tal y como dice en el paper de llama\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_toTrain=model"
      ],
      "metadata": {
        "id": "FXaPR1AhKI17"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-06T09:03:46.487132Z",
          "iopub.status.busy": "2023-08-06T09:03:46.487014Z",
          "iopub.status.idle": "2023-08-06T09:03:46.490242Z",
          "shell.execute_reply": "2023-08-06T09:03:46.489500Z",
          "shell.execute_reply.started": "2023-08-06T09:03:46.487132Z"
        },
        "id": "Iriilr4219PU"
      },
      "outputs": [],
      "source": [
        "os.environ['WANDB_API_KEY'] = '4568bca12d8724d7cd88b0902226349c1d621364'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-06T09:03:46.492228Z",
          "iopub.status.busy": "2023-08-06T09:03:46.492088Z",
          "iopub.status.idle": "2023-08-06T09:03:46.608916Z",
          "shell.execute_reply": "2023-08-06T09:03:46.608234Z",
          "shell.execute_reply.started": "2023-08-06T09:03:46.492225Z"
        },
        "id": "pRWNx5vlDmRb"
      },
      "outputs": [],
      "source": [
        "response_template_with_context = \"\\n### Emocion:\"\n",
        "response_template_ids = tokenizer.encode(response_template_with_context, add_special_tokens=False)[2:]  # Now we have it like in the dataset texts: `[2277, 29937, 4007, 22137, 29901]`\n",
        "print(response_template_ids)\n",
        "collator = DataCollatorForCompletionOnlyLM(response_template_ids, tokenizer=tokenizer)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = SFTTrainer(\n",
        "    model=model_toTrain,\n",
        "    data_collator= collator,\n",
        "    train_dataset=train_data,\n",
        "    eval_dataset =val_data,\n",
        "    peft_config=config,\n",
        "    dataset_text_field=\"Prompt\",\n",
        "    max_seq_length=1024,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_arguments,\n",
        ")"
      ],
      "metadata": {
        "id": "rNj2sRxFKEbO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-06T09:03:46.650960Z",
          "iopub.status.busy": "2023-08-06T09:03:46.650732Z",
          "iopub.status.idle": "2023-08-06T12:18:57.343931Z",
          "shell.execute_reply": "2023-08-06T12:18:57.343270Z",
          "shell.execute_reply.started": "2023-08-06T09:03:46.650900Z"
        },
        "id": "4SpHp6Fa19PV"
      },
      "outputs": [],
      "source": [
        "trainer.train(resume_from_checkpoint=True) # se entrena el modelo\n",
        "trainer.save_model(\"/content/gdrive/MyDrive/Tesis/Modelos/LLaMA2_models\") # se guarda el modelo"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "1OEYS8-onJVh"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}